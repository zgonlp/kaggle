{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled review\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('labeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "test = pd.read_csv('testData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)\n",
    "\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled review\\n\"%(train['review'].size,test['review'].size,unlabeled_train['review'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words. Return a list of words.\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    review_text = re.sub(\"[^a-zA-Z1-9]\",\" \",review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "# Use NLTK's punkt tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Return a list of sentences, where each sentence is a list of words\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0 :\n",
    "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/yangjun/anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paring sentences from unlabelled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print('Parsing sentences from training set')\n",
    "for review in train['review']:\n",
    "    sentences += review_to_sentences(review,tokenizer)\n",
    "    \n",
    "print(\"Paring sentences from unlabelled set\")\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review,tokenizer)\n",
    "# \"append\" will only append the first list\n",
    "# \"+=\" will join all of the lists at once\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-08 11:20:33,796:INFO: collecting all words and their counts\n",
      "2017-11-08 11:20:33,797:INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-08 11:20:33,867:INFO: PROGRESS: at sentence #10000, processed 227353 words, keeping 17965 word types\n",
      "2017-11-08 11:20:33,960:INFO: PROGRESS: at sentence #20000, processed 454813 words, keeping 25212 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-08 11:20:34,014:INFO: PROGRESS: at sentence #30000, processed 675603 words, keeping 30355 word types\n",
      "2017-11-08 11:20:34,087:INFO: PROGRESS: at sentence #40000, processed 903462 words, keeping 34720 word types\n",
      "2017-11-08 11:20:34,135:INFO: PROGRESS: at sentence #50000, processed 1124077 words, keeping 38170 word types\n",
      "2017-11-08 11:20:34,181:INFO: PROGRESS: at sentence #60000, processed 1346959 words, keeping 41167 word types\n",
      "2017-11-08 11:20:34,230:INFO: PROGRESS: at sentence #70000, processed 1571546 words, keeping 43800 word types\n",
      "2017-11-08 11:20:34,279:INFO: PROGRESS: at sentence #80000, processed 1792149 words, keeping 46207 word types\n",
      "2017-11-08 11:20:34,332:INFO: PROGRESS: at sentence #90000, processed 2017757 words, keeping 48659 word types\n",
      "2017-11-08 11:20:34,388:INFO: PROGRESS: at sentence #100000, processed 2241043 words, keeping 50755 word types\n",
      "2017-11-08 11:20:34,446:INFO: PROGRESS: at sentence #110000, processed 2462195 words, keeping 52649 word types\n",
      "2017-11-08 11:20:34,494:INFO: PROGRESS: at sentence #120000, processed 2685717 words, keeping 54720 word types\n",
      "2017-11-08 11:20:34,546:INFO: PROGRESS: at sentence #130000, processed 2912792 words, keeping 56479 word types\n",
      "2017-11-08 11:20:34,597:INFO: PROGRESS: at sentence #140000, processed 3126918 words, keeping 58007 word types\n",
      "2017-11-08 11:20:34,644:INFO: PROGRESS: at sentence #150000, processed 3353969 words, keeping 59742 word types\n",
      "2017-11-08 11:20:34,692:INFO: PROGRESS: at sentence #160000, processed 3577978 words, keeping 61324 word types\n",
      "2017-11-08 11:20:34,736:INFO: PROGRESS: at sentence #170000, processed 3802724 words, keeping 62811 word types\n",
      "2017-11-08 11:20:34,784:INFO: PROGRESS: at sentence #180000, processed 4024729 words, keeping 64249 word types\n",
      "2017-11-08 11:20:34,832:INFO: PROGRESS: at sentence #190000, processed 4251346 words, keeping 65560 word types\n",
      "2017-11-08 11:20:34,879:INFO: PROGRESS: at sentence #200000, processed 4476898 words, keeping 66871 word types\n",
      "2017-11-08 11:20:34,926:INFO: PROGRESS: at sentence #210000, processed 4699798 words, keeping 68192 word types\n",
      "2017-11-08 11:20:34,973:INFO: PROGRESS: at sentence #220000, processed 4926237 words, keeping 69520 word types\n",
      "2017-11-08 11:20:35,020:INFO: PROGRESS: at sentence #230000, processed 5150232 words, keeping 70796 word types\n",
      "2017-11-08 11:20:35,068:INFO: PROGRESS: at sentence #240000, processed 5379182 words, keeping 72024 word types\n",
      "2017-11-08 11:20:35,113:INFO: PROGRESS: at sentence #250000, processed 5594663 words, keeping 73230 word types\n",
      "2017-11-08 11:20:35,160:INFO: PROGRESS: at sentence #260000, processed 5816026 words, keeping 74377 word types\n",
      "2017-11-08 11:20:35,207:INFO: PROGRESS: at sentence #270000, processed 6038789 words, keeping 75694 word types\n",
      "2017-11-08 11:20:35,257:INFO: PROGRESS: at sentence #280000, processed 6266050 words, keeping 77312 word types\n",
      "2017-11-08 11:20:35,305:INFO: PROGRESS: at sentence #290000, processed 6490580 words, keeping 78798 word types\n",
      "2017-11-08 11:20:35,355:INFO: PROGRESS: at sentence #300000, processed 6716640 words, keeping 80147 word types\n",
      "2017-11-08 11:20:35,405:INFO: PROGRESS: at sentence #310000, processed 6943341 words, keeping 81480 word types\n",
      "2017-11-08 11:20:35,506:INFO: PROGRESS: at sentence #320000, processed 7169590 words, keeping 82829 word types\n",
      "2017-11-08 11:20:35,559:INFO: PROGRESS: at sentence #330000, processed 7392774 words, keeping 84079 word types\n",
      "2017-11-08 11:20:35,608:INFO: PROGRESS: at sentence #340000, processed 7623693 words, keeping 85353 word types\n",
      "2017-11-08 11:20:35,662:INFO: PROGRESS: at sentence #350000, processed 7848385 words, keeping 86522 word types\n",
      "2017-11-08 11:20:35,722:INFO: PROGRESS: at sentence #360000, processed 8070301 words, keeping 87713 word types\n",
      "2017-11-08 11:20:35,769:INFO: PROGRESS: at sentence #370000, processed 8298859 words, keeping 88842 word types\n",
      "2017-11-08 11:20:35,830:INFO: PROGRESS: at sentence #380000, processed 8525412 words, keeping 90026 word types\n",
      "2017-11-08 11:20:35,880:INFO: PROGRESS: at sentence #390000, processed 8756612 words, keeping 91073 word types\n",
      "2017-11-08 11:20:35,929:INFO: PROGRESS: at sentence #400000, processed 8980975 words, keeping 92111 word types\n",
      "2017-11-08 11:20:35,977:INFO: PROGRESS: at sentence #410000, processed 9203716 words, keeping 93094 word types\n",
      "2017-11-08 11:20:36,026:INFO: PROGRESS: at sentence #420000, processed 9426216 words, keeping 94141 word types\n",
      "2017-11-08 11:20:36,078:INFO: PROGRESS: at sentence #430000, processed 9655281 words, keeping 95178 word types\n",
      "2017-11-08 11:20:36,132:INFO: PROGRESS: at sentence #440000, processed 9883539 words, keeping 96170 word types\n",
      "2017-11-08 11:20:36,182:INFO: PROGRESS: at sentence #450000, processed 10108632 words, keeping 97322 word types\n",
      "2017-11-08 11:20:36,242:INFO: PROGRESS: at sentence #460000, processed 10342882 words, keeping 98393 word types\n",
      "2017-11-08 11:20:36,321:INFO: PROGRESS: at sentence #470000, processed 10572174 words, keeping 99263 word types\n",
      "2017-11-08 11:20:36,414:INFO: PROGRESS: at sentence #480000, processed 10793924 words, keeping 100209 word types\n",
      "2017-11-08 11:20:36,496:INFO: PROGRESS: at sentence #490000, processed 11022214 words, keeping 101275 word types\n",
      "2017-11-08 11:20:36,572:INFO: PROGRESS: at sentence #500000, processed 11245293 words, keeping 102190 word types\n",
      "2017-11-08 11:20:36,648:INFO: PROGRESS: at sentence #510000, processed 11472001 words, keeping 103137 word types\n",
      "2017-11-08 11:20:36,713:INFO: PROGRESS: at sentence #520000, processed 11696820 words, keeping 104050 word types\n",
      "2017-11-08 11:20:36,780:INFO: PROGRESS: at sentence #530000, processed 11922646 words, keeping 104875 word types\n",
      "2017-11-08 11:20:36,846:INFO: PROGRESS: at sentence #540000, processed 12148743 words, keeping 105770 word types\n",
      "2017-11-08 11:20:36,911:INFO: PROGRESS: at sentence #550000, processed 12375685 words, keeping 106655 word types\n",
      "2017-11-08 11:20:36,979:INFO: PROGRESS: at sentence #560000, processed 12598355 words, keeping 107532 word types\n",
      "2017-11-08 11:20:37,047:INFO: PROGRESS: at sentence #570000, processed 12828819 words, keeping 108334 word types\n",
      "2017-11-08 11:20:37,117:INFO: PROGRESS: at sentence #580000, processed 13051672 words, keeping 109220 word types\n",
      "2017-11-08 11:20:37,181:INFO: PROGRESS: at sentence #590000, processed 13278556 words, keeping 110082 word types\n",
      "2017-11-08 11:20:37,251:INFO: PROGRESS: at sentence #600000, processed 13502204 words, keeping 110826 word types\n",
      "2017-11-08 11:20:37,317:INFO: PROGRESS: at sentence #610000, processed 13724633 words, keeping 111722 word types\n",
      "2017-11-08 11:20:37,391:INFO: PROGRESS: at sentence #620000, processed 13952494 words, keeping 112490 word types\n",
      "2017-11-08 11:20:37,465:INFO: PROGRESS: at sentence #630000, processed 14178179 words, keeping 113282 word types\n",
      "2017-11-08 11:20:37,540:INFO: PROGRESS: at sentence #640000, processed 14400319 words, keeping 114110 word types\n",
      "2017-11-08 11:20:37,633:INFO: PROGRESS: at sentence #650000, processed 14627631 words, keeping 114910 word types\n",
      "2017-11-08 11:20:37,710:INFO: PROGRESS: at sentence #660000, processed 14851823 words, keeping 115678 word types\n",
      "2017-11-08 11:20:37,800:INFO: PROGRESS: at sentence #670000, processed 15076583 words, keeping 116390 word types\n",
      "2017-11-08 11:20:37,876:INFO: PROGRESS: at sentence #680000, processed 15302872 words, keeping 117111 word types\n",
      "2017-11-08 11:20:37,946:INFO: PROGRESS: at sentence #690000, processed 15526415 words, keeping 117904 word types\n",
      "2017-11-08 11:20:38,017:INFO: PROGRESS: at sentence #700000, processed 15756467 words, keeping 118729 word types\n",
      "2017-11-08 11:20:38,083:INFO: PROGRESS: at sentence #710000, processed 15980848 words, keeping 119388 word types\n",
      "2017-11-08 11:20:38,159:INFO: PROGRESS: at sentence #720000, processed 16207530 words, keeping 120021 word types\n",
      "2017-11-08 11:20:38,255:INFO: PROGRESS: at sentence #730000, processed 16435247 words, keeping 120766 word types\n",
      "2017-11-08 11:20:38,347:INFO: PROGRESS: at sentence #740000, processed 16657712 words, keeping 121488 word types\n",
      "2017-11-08 11:20:38,426:INFO: PROGRESS: at sentence #750000, processed 16877511 words, keeping 122132 word types\n",
      "2017-11-08 11:20:38,480:INFO: PROGRESS: at sentence #760000, processed 17098300 words, keeping 122775 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-08 11:20:38,539:INFO: PROGRESS: at sentence #770000, processed 17326890 words, keeping 123559 word types\n",
      "2017-11-08 11:20:38,592:INFO: PROGRESS: at sentence #780000, processed 17558529 words, keeping 124280 word types\n",
      "2017-11-08 11:20:38,640:INFO: PROGRESS: at sentence #790000, processed 17786955 words, keeping 124962 word types\n",
      "2017-11-08 11:20:38,668:INFO: collected 125407 word types from a corpus of 17910811 raw words and 795538 sentences\n",
      "2017-11-08 11:20:38,669:INFO: Loading a fresh vocabulary\n",
      "2017-11-08 11:20:39,788:INFO: min_count=40 retains 16670 unique words (13% of original 125407, drops 108737)\n",
      "2017-11-08 11:20:39,789:INFO: min_count=40 leaves 17346833 word corpus (96% of original 17910811, drops 563978)\n",
      "2017-11-08 11:20:39,842:INFO: deleting the raw counts dictionary of 125407 items\n",
      "2017-11-08 11:20:39,847:INFO: sample=0.001 downsamples 48 most-common words\n",
      "2017-11-08 11:20:39,848:INFO: downsampling leaves estimated 12870163 word corpus (74.2% of prior 17346833)\n",
      "2017-11-08 11:20:39,849:INFO: estimated required memory for 16670 words and 300 dimensions: 48343000 bytes\n",
      "2017-11-08 11:20:39,914:INFO: resetting layer weights\n",
      "2017-11-08 11:20:40,194:INFO: training model with 4 workers on 16670 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=40\n",
      "2017-11-08 11:20:41,218:INFO: PROGRESS: at 0.85% examples, 535830 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:42,224:INFO: PROGRESS: at 1.77% examples, 560034 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:43,234:INFO: PROGRESS: at 2.78% examples, 586736 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:44,234:INFO: PROGRESS: at 3.77% examples, 597662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:45,248:INFO: PROGRESS: at 4.62% examples, 585602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:46,265:INFO: PROGRESS: at 5.60% examples, 591423 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:47,269:INFO: PROGRESS: at 6.70% examples, 605724 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:48,274:INFO: PROGRESS: at 7.80% examples, 618394 words/s, in_qsize 7, out_qsize 1\n",
      "2017-11-08 11:20:49,285:INFO: PROGRESS: at 8.92% examples, 629263 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:50,288:INFO: PROGRESS: at 10.02% examples, 637748 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:51,293:INFO: PROGRESS: at 11.13% examples, 643930 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:52,304:INFO: PROGRESS: at 12.18% examples, 647000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:53,315:INFO: PROGRESS: at 13.25% examples, 649646 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:54,316:INFO: PROGRESS: at 14.35% examples, 653903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:55,333:INFO: PROGRESS: at 15.49% examples, 658333 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:56,335:INFO: PROGRESS: at 16.61% examples, 661885 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:57,352:INFO: PROGRESS: at 17.66% examples, 662341 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:58,354:INFO: PROGRESS: at 18.77% examples, 665260 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:20:59,363:INFO: PROGRESS: at 19.88% examples, 667698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:00,377:INFO: PROGRESS: at 21.01% examples, 670000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:01,389:INFO: PROGRESS: at 22.06% examples, 669511 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-08 11:21:02,407:INFO: PROGRESS: at 23.02% examples, 666603 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:03,419:INFO: PROGRESS: at 24.04% examples, 665329 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:04,433:INFO: PROGRESS: at 25.03% examples, 663837 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:05,459:INFO: PROGRESS: at 25.96% examples, 660720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:06,466:INFO: PROGRESS: at 27.00% examples, 660472 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:07,468:INFO: PROGRESS: at 27.99% examples, 659600 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:08,472:INFO: PROGRESS: at 29.04% examples, 660055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:09,474:INFO: PROGRESS: at 30.14% examples, 661927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:10,477:INFO: PROGRESS: at 31.25% examples, 663692 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:11,483:INFO: PROGRESS: at 32.34% examples, 665050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:12,485:INFO: PROGRESS: at 33.43% examples, 666174 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:13,490:INFO: PROGRESS: at 34.53% examples, 667398 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:14,497:INFO: PROGRESS: at 35.65% examples, 668735 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:15,501:INFO: PROGRESS: at 36.77% examples, 670020 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:16,503:INFO: PROGRESS: at 37.88% examples, 671287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:17,512:INFO: PROGRESS: at 38.98% examples, 672181 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:18,518:INFO: PROGRESS: at 40.08% examples, 673068 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:19,522:INFO: PROGRESS: at 41.19% examples, 674111 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:20,523:INFO: PROGRESS: at 42.32% examples, 675169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:21,525:INFO: PROGRESS: at 43.44% examples, 675979 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:22,537:INFO: PROGRESS: at 44.57% examples, 676932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:23,547:INFO: PROGRESS: at 45.66% examples, 677345 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:24,550:INFO: PROGRESS: at 46.78% examples, 678006 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:25,555:INFO: PROGRESS: at 47.88% examples, 678662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:26,556:INFO: PROGRESS: at 48.98% examples, 679328 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:27,561:INFO: PROGRESS: at 50.07% examples, 679898 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:28,562:INFO: PROGRESS: at 51.17% examples, 680493 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:29,563:INFO: PROGRESS: at 52.25% examples, 680922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:30,569:INFO: PROGRESS: at 53.35% examples, 681443 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:31,577:INFO: PROGRESS: at 54.44% examples, 681746 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:32,585:INFO: PROGRESS: at 55.53% examples, 682055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:33,595:INFO: PROGRESS: at 56.65% examples, 682586 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:34,602:INFO: PROGRESS: at 57.75% examples, 682998 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:35,606:INFO: PROGRESS: at 58.86% examples, 683440 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:36,611:INFO: PROGRESS: at 59.94% examples, 683732 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:37,618:INFO: PROGRESS: at 61.03% examples, 683965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:38,620:INFO: PROGRESS: at 62.16% examples, 684512 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:39,623:INFO: PROGRESS: at 63.25% examples, 684795 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:40,626:INFO: PROGRESS: at 64.37% examples, 685176 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:41,636:INFO: PROGRESS: at 65.47% examples, 685338 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:42,642:INFO: PROGRESS: at 66.59% examples, 685776 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:43,643:INFO: PROGRESS: at 67.68% examples, 686043 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:44,643:INFO: PROGRESS: at 68.77% examples, 686298 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:45,644:INFO: PROGRESS: at 69.85% examples, 686546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:46,644:INFO: PROGRESS: at 70.95% examples, 686898 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:47,648:INFO: PROGRESS: at 72.04% examples, 687196 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:48,650:INFO: PROGRESS: at 73.14% examples, 687412 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:49,650:INFO: PROGRESS: at 74.24% examples, 687740 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-08 11:21:50,652:INFO: PROGRESS: at 75.34% examples, 688044 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:51,662:INFO: PROGRESS: at 76.45% examples, 688354 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:52,664:INFO: PROGRESS: at 77.54% examples, 688538 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:53,668:INFO: PROGRESS: at 78.64% examples, 688789 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:54,681:INFO: PROGRESS: at 79.76% examples, 689063 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-08 11:21:55,688:INFO: PROGRESS: at 80.87% examples, 689370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:56,696:INFO: PROGRESS: at 82.00% examples, 689658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:57,698:INFO: PROGRESS: at 83.09% examples, 689815 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:58,706:INFO: PROGRESS: at 84.20% examples, 689915 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:21:59,723:INFO: PROGRESS: at 85.33% examples, 690204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:00,729:INFO: PROGRESS: at 86.45% examples, 690470 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:01,750:INFO: PROGRESS: at 87.57% examples, 690625 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:02,757:INFO: PROGRESS: at 88.66% examples, 690805 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:03,759:INFO: PROGRESS: at 89.75% examples, 690930 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:04,774:INFO: PROGRESS: at 90.86% examples, 691114 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:05,776:INFO: PROGRESS: at 91.96% examples, 691394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:06,780:INFO: PROGRESS: at 93.06% examples, 691576 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:07,783:INFO: PROGRESS: at 94.17% examples, 691853 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:08,788:INFO: PROGRESS: at 95.29% examples, 692099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:09,794:INFO: PROGRESS: at 96.40% examples, 692334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:10,809:INFO: PROGRESS: at 97.52% examples, 692492 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:11,821:INFO: PROGRESS: at 98.63% examples, 692670 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:12,827:INFO: PROGRESS: at 99.74% examples, 692900 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-08 11:22:13,035:INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-08 11:22:13,049:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-08 11:22:13,067:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-08 11:22:13,073:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-08 11:22:13,074:INFO: training on 89554055 raw words (64347216 effective words) took 92.9s, 692857 effective words/s\n",
      "2017-11-08 11:22:13,075:INFO: precomputing L2-norms of word weight vectors\n",
      "2017-11-08 11:22:13,221:INFO: saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-11-08 11:22:13,221:INFO: not storing attribute syn0norm\n",
      "2017-11-08 11:22:13,222:INFO: not storing attribute cum_table\n",
      "2017-11-08 11:22:13,714:INFO: saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s: %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count, helps limit the size of the vocabulary to meaningful words\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "context = 40 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import Word2Vec\n",
    "print(\"Training the model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features, min_count=min_word_count,\\\n",
    "                          window = context, sample=downsampling)\n",
    "\n",
    "model.init_sims(replace=True) # make the model more memory-efficient\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Words To Paragraphs : Vector Averaging\n",
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words,model,num_features):\n",
    "    # Funciton to averaging all of the word vectors in a given paragraph\n",
    "    featureVec=np.zeros((num_features,),dtype='float32')\n",
    "    \n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords+=1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "            \n",
    "    # Divide the result by the number of vector to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews,model,num_features):\n",
    "    # Given a set of reviews(each one a list of words),calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    \n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "    \n",
    "    for review in reviews:\n",
    "        \n",
    "        if counter%5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review,model,num_features)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangjun/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/yangjun/anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Creating average feature vecs fro test reviews\n",
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(review_to_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews,model,num_features)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Creating average feature vecs fro test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "testDataVecs=getAvgFeatureVecs(clean_test_reviews,model,num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labelled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data,using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "print(\"Fitting a random forest to labelled training data...\")\n",
    "forest = forest.fit(trainDataVecs,train['sentiment'])\n",
    "\n",
    "# Test & extract results\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame(data={'id':test['id'],'sentiment':result})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False,quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dab4536fb02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Initalize a k-means object and use it to extract centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mkmeans_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans_clustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get the end time and print how long the process took\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[0;32m--> 394\u001b[0;31m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[0;32m--> 681\u001b[0;31m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangjun/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_k_init\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mx_squared_norms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_squared_norms None in _k_init'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
